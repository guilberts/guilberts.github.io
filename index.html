<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Guilbert's Portfolio</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 40px; text-align: center; }
    h1 { color: #2c3e50; }
    p,li { color: #555; }
    li {text-align:left;
        line-height:24px;  }

  </style>
</head>
<body>
  <h1>Welcome to My Portfolio</h1>
  <p>Hi, I'm Guilbert. This is my portfolio page hosted on GitHub Pages ðŸš€</p>
 <!-- Lista Ordenada (ol) -->
  <br>
<ul>
  <li><p>Here I'm sharing a simple example of transforming data using PySpark, be in mind it's just a sample.
        I took advantage and build a automate test the dockerimage and CI using Github Actions follow the link
        to check it out <a href="https://github.com/guilberts/data-engineering-portfolio/blob/main/README.md">ETL with PySpark (Simulation)</a></p> 
    </li>
  <li><p>This project demonstrates a simplified data pipeline that simulates a job using PySpark and Apache Kafka.
        It generates data dynamically, pushes it to a Kafka topic, then consumes the data back from the same topic. The output is displayed on the console, 
        but the workflow can easily be extended to include data transformations and storage in other formats such as Parquet, CSV, or even insertion into a database.
        Check it out <a href="https://github.com/guilberts/data-engineering-read-data-with-spark-kafka/tree/main">ETL with PySpark (Simulation)</a></p> 
    </li>
  <li>Comming soon..</li>
</ul>
  </body>
</html>
